###################################################
# Adapted from KillrWeather Reference Config File #
# Thanks to Helena Edelson
###################################################

spark {
  # The Spark master host. Set first by environment if exists. Then system, then config.
  # Options: spark://host1:port1,host2:port2
  # - "local" to run locally with one thread,
  # - local[4]" to run locally with 4 cores
  # - the master IP/hostname to run on a Spark standalone cluster
  # - if not set, defaults to "local[*]" to run with enough threads
  # Supports optional HA failover for more than one: host1,host2..
  # which is used to inform: spark://host1:port1,host2:port2
  master = "spark://127.0.0.1:7077"

  # The batch interval must be set based on the latency requirements
  # of your application and available cluster resources.
  streaming.batch.interval = 500

  cleaner.ttl.seconds = 600
}

kafka {
  hosts = [${?KAFKA_HOSTS}]
  ingest-rate = 1s

  zookeeper {
    connection = ""
    port = 2181
  }
  group.id = "cc-demo"
  topic.raw = "cc-demo"
  partitioner.fqcn = "kafka.producer.DefaultPartitioner"
  encoder.fqcn = "kafka.serializer.StringEncoder"
  decoder.fqcn = "kafka.serializer.StringDecoder"
  batch.send.size = 10
}


# Undefined values are expected to be set by the user in their application.conf file
# Defined values are defaults which can be overridden by the user in their application.conf file
# We first check for a setting in the environment deployed to, then whether a java system property
# exists to use, and finally the configured one. The users configured values would override
# any configured setting in this file.
# For environment keys, set the key name noted, for instance: ${?CASSANDRA_KEYSPACE}, your
# environment should be CASSANDRA_KEYSPACE="my_keyspace" or CASSANDRA_RPC_PORT=9161
cassandra {

  # The contact point to connect to the Cassandra cluster.
  # Accepts a comma-separated string of hosts. Override with -Dcassandra.connection.host.
  connection.host = "192.168.59.104"

  keyspace.name = "datastax_spark_cc_demo"

  rawtable.name = "credit_card_transactions"

  aggregatetable.name = "periodic_merchant_rollup"


  ## Tuning ##

  # The number of milliseconds to keep unused `Cluster` object before destroying it
  # The duration to keep unused connections open. In millis, defaults to 250.
  # Override with -Dcassandra.connection.keep_alive_ms.
  connection.keep-alive = ${?CASSANDRA_KEEP_ALIVE_MS}

  # The number of times to retry a failed query. Defaults to 10.
  # Override with -Dcassandra.query.retry.count.
  connection.query.retry.count = ${?CASSANDRA_QUEUE_RETRY_COUNT}

  # The initial delay determining how often to try to reconnect to a dead node. In millis, defaults to 1000.
  # Override with -Dcassandra.connection.reconnection_delay_ms.min.
  connection.reconnect-delay.min = ${?CASSANDRA_MIN_RECONNECT_DELAY_MS}

  # The final delay determining how often to try to reconnect to a dead node. In millis, defaults to 60000.
  # Override with -Dcassandra.connection.reconnection_delay_ms.max.
  connection.reconnect-delay.max = ${?CASSANDRA_MAX_RECONNECT_DELAY_MS}

  ## Tuning: use to fine-tune the read process ##

  # To reduce the number of roundtrips to Cassandra, partitions are paged
  # The following properties control the number of partitions and the fetch size:
  # The number of rows fetched per roundtrip. Defaults to 1000.
  # Override with -Dcassandra.input.page.row.size
  read.page.row.size = ${?CASSANDRA_READ_PAGE_ROW_SIZE}

  # How many rows to fetch in a single task. Defaults to 100000.
  # Override with -Dcassandra.input.split.size
  read.split.size = ${?CASSANDRA_READ_SPLIT_SIZE}

  # The consistency level to use when reading. By default, reads are performed at
  # ConsistencyLevel.LOCAL_ONE in order to leverage data-locality and minimize network traffic.
  # Override with -Dcassandra.input.consistency.level.
  read.consistency.level = ${?CASSANDRA_READ_CONSISTENCY_LEVEL}


  ## Tuning: use to fine-tune the saving process ##

  # The maximum number of batches executed in parallel by a single task.
  # Defaults to 5. Override with -Dcassandra.output.concurrent.writes.
  write.concurrent.writes = ${?CASSANDRA_WRITE_CONCURRENT_WRITES}

  # The maximum total size of the batch in bytes; defaults to 64 kB.
  # Override with -Dcassandra.output.batch.size.bytes.
  write.batch.size.bytes = ${?CASSANDRA_WRITE_BATCH_SIZE_BYTES}

  # The number of rows per single batch; default is 'auto' which means the driver
  # will adjust the number of rows based on the amount of data in each row.
  # Override with -Dcassandra.output.batch.size.rows.
  write.batch.size.rows = ${?CASSANDRA_WRITE_BATCH_SIZE_ROWS}

  # The maximum total size of the batch in bytes. Defaults to 64 kB.
  # Override with -D
  write.max-bytes = ${?CASSANDRA_WRITE_MAX_BYTES}

  # By default, writes are performed at ConsistencyLevel.ONE in order to leverage data-locality
  # and minimize network traffic. Override with -Dcassandra.output.consistency.level
  write.consistency.level = ${?CASSANDRA_READ_CONSISTENCY_LEVEL}
}

akka {

  loglevel = "DEBUG"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = on
  log-dead-letters-during-shutdown = on

  remote {
    log-remote-lifecycle-events = off
    netty.tcp.port = 2550
  }

}


